{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1SUdjiMjVhF"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import os  # For file and directory operations\n",
        "import librosa  # For audio processing\n",
        "from keras.models import Model  # For creating the Keras model\n",
        "from keras.layers import Input, LSTM, Dense, Dropout, Activation, Bidirectional  # For layers in the model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam  # For the optimizer\n",
        "from tensorflow.keras.utils import to_categorical  # For one-hot encoding\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7wfgh1DGWZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-m1FDT3HjV3q"
      },
      "outputs": [],
      "source": [
        "# Function to extract MFCC features from an audio file\n",
        "def mfcc_extract(wavfile_name):\n",
        "    y, sr = librosa.load(wavfile_name)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    return mfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "p5m4XBlbjWiH",
        "outputId": "4981f77f-56bf-4761-a20a-acfe482d5c80"
      },
      "outputs": [],
      "source": [
        "# Initialize lists for storing data and labels\n",
        "emotion_labels = []\n",
        "gender_labels = []\n",
        "data = []\n",
        "\n",
        "# Loop through each file in the dataset directory\n",
        "for dirname, _, filenames in os.walk('d:/PROJECTS/SIH_HACKATHON/ravdess_dataset/data'):\n",
        "    for filename in filenames:\n",
        "        try:\n",
        "            actor_number = int(filename.split(\"-\")[-1].split(\".\")[0])\n",
        "            emotion_labels.append(int(filename[7:8]) - 1)\n",
        "            gender_labels.append(0 if actor_number % 2 == 0 else 1)\n",
        "            wavfile_name = os.path.join(dirname, filename)\n",
        "            data.append(mfcc_extract(wavfile_name))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed path: {wavfile_name}, Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFPdS81A90ak"
      },
      "outputs": [],
      "source": [
        "0# Convert lists to numpy arrays for use in ML\n",
        "data_array = np.asarray(data)\n",
        "emotion_label_array = np.array(emotion_labels)\n",
        "gender_label_array = np.array(gender_labels)\n",
        "\n",
        "# One-hot encode the labels\n",
        "emotion_label_categorical = to_categorical(emotion_label_array)\n",
        "gender_label_categorical = to_categorical(gender_label_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCPRMI1HjW1k"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training, validation, and test sets\n",
        "x_data, x_test, y_data_emotion, y_test_emotion, y_data_gender, y_test_gender = train_test_split(\n",
        "    np.array(data_array), emotion_label_categorical, gender_label_categorical, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train, x_val, y_train_emotion, y_val_emotion, y_train_gender, y_val_gender = train_test_split(\n",
        "    x_data, y_data_emotion, y_data_gender, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKvoKKVNjXcJ"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model with two bidirectional LSTM layers\n",
        "def lstm_model():\n",
        "    inp = Input(shape=(40, 1))  # Input layer\n",
        "\n",
        "    # First Bidirectional LSTM layer\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n",
        "\n",
        "    # Second Bidirectional LSTM layer\n",
        "    x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
        "\n",
        "    x = Dense(128)(x)  # Dense layer\n",
        "    x = Dropout(0.4)(x)  # Dropout layer for regularization\n",
        "    x = Activation('relu')(x)  # ReLU activation\n",
        "\n",
        "    x = Dense(128)(x)  # Another dense layer\n",
        "    x = Dropout(0.4)(x)  # Another dropout layer for regularization\n",
        "    x = Activation('relu')(x)  # Another ReLU activation\n",
        "\n",
        "    # Output layers for emotion and gender\n",
        "    emotion_output = Dense(8, activation='softmax', name='emotion_output')(x)\n",
        "    gender_output = Dense(2, activation='softmax', name='gender_output')(x)\n",
        "\n",
        "    # Final model\n",
        "    model = Model(inputs=inp, outputs=[emotion_output, gender_output])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss={\"emotion_output\": \"categorical_crossentropy\", \"gender_output\": \"categorical_crossentropy\"},\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J20P80KY-tG5"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=90)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksfooFPO-yR8",
        "outputId": "0ac945d2-96af-407e-85ad-3a5199de2627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "54/54 [==============================] - 11s 45ms/step - loss: 2.5017 - emotion_output_loss: 2.0587 - gender_output_loss: 0.4430 - emotion_output_accuracy: 0.1714 - gender_output_accuracy: 0.8054 - val_loss: 2.3356 - val_emotion_output_loss: 1.9706 - val_gender_output_loss: 0.3650 - val_emotion_output_accuracy: 0.1875 - val_gender_output_accuracy: 0.8455 - lr: 0.0010\n",
            "Epoch 2/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 2.3734 - emotion_output_loss: 2.0035 - gender_output_loss: 0.3699 - emotion_output_accuracy: 0.1922 - gender_output_accuracy: 0.8332 - val_loss: 2.2552 - val_emotion_output_loss: 1.9254 - val_gender_output_loss: 0.3298 - val_emotion_output_accuracy: 0.2639 - val_gender_output_accuracy: 0.8472 - lr: 0.0010\n",
            "Epoch 3/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 2.2578 - emotion_output_loss: 1.9192 - gender_output_loss: 0.3387 - emotion_output_accuracy: 0.2345 - gender_output_accuracy: 0.8570 - val_loss: 2.1706 - val_emotion_output_loss: 1.8581 - val_gender_output_loss: 0.3125 - val_emotion_output_accuracy: 0.2847 - val_gender_output_accuracy: 0.8611 - lr: 0.0010\n",
            "Epoch 4/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 2.1897 - emotion_output_loss: 1.8630 - gender_output_loss: 0.3266 - emotion_output_accuracy: 0.2710 - gender_output_accuracy: 0.8599 - val_loss: 2.1079 - val_emotion_output_loss: 1.7878 - val_gender_output_loss: 0.3201 - val_emotion_output_accuracy: 0.3142 - val_gender_output_accuracy: 0.8455 - lr: 0.0010\n",
            "Epoch 5/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 2.1284 - emotion_output_loss: 1.8226 - gender_output_loss: 0.3058 - emotion_output_accuracy: 0.2994 - gender_output_accuracy: 0.8622 - val_loss: 2.0098 - val_emotion_output_loss: 1.7355 - val_gender_output_loss: 0.2743 - val_emotion_output_accuracy: 0.3264 - val_gender_output_accuracy: 0.8611 - lr: 0.0010\n",
            "Epoch 6/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 2.0294 - emotion_output_loss: 1.7362 - gender_output_loss: 0.2933 - emotion_output_accuracy: 0.3237 - gender_output_accuracy: 0.8743 - val_loss: 1.9450 - val_emotion_output_loss: 1.6729 - val_gender_output_loss: 0.2721 - val_emotion_output_accuracy: 0.3455 - val_gender_output_accuracy: 0.8819 - lr: 0.0010\n",
            "Epoch 7/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 2.0329 - emotion_output_loss: 1.7199 - gender_output_loss: 0.3130 - emotion_output_accuracy: 0.3393 - gender_output_accuracy: 0.8651 - val_loss: 1.9370 - val_emotion_output_loss: 1.6652 - val_gender_output_loss: 0.2718 - val_emotion_output_accuracy: 0.3715 - val_gender_output_accuracy: 0.8854 - lr: 0.0010\n",
            "Epoch 8/250\n",
            "54/54 [==============================] - 1s 26ms/step - loss: 1.9741 - emotion_output_loss: 1.7032 - gender_output_loss: 0.2708 - emotion_output_accuracy: 0.3301 - gender_output_accuracy: 0.8813 - val_loss: 1.9231 - val_emotion_output_loss: 1.6509 - val_gender_output_loss: 0.2722 - val_emotion_output_accuracy: 0.3681 - val_gender_output_accuracy: 0.8924 - lr: 0.0010\n",
            "Epoch 9/250\n",
            "54/54 [==============================] - 1s 26ms/step - loss: 1.9378 - emotion_output_loss: 1.6749 - gender_output_loss: 0.2629 - emotion_output_accuracy: 0.3561 - gender_output_accuracy: 0.8859 - val_loss: 1.9026 - val_emotion_output_loss: 1.6430 - val_gender_output_loss: 0.2596 - val_emotion_output_accuracy: 0.3611 - val_gender_output_accuracy: 0.8854 - lr: 0.0010\n",
            "Epoch 10/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.8598 - emotion_output_loss: 1.6158 - gender_output_loss: 0.2439 - emotion_output_accuracy: 0.3596 - gender_output_accuracy: 0.8929 - val_loss: 1.7738 - val_emotion_output_loss: 1.5390 - val_gender_output_loss: 0.2348 - val_emotion_output_accuracy: 0.4167 - val_gender_output_accuracy: 0.9080 - lr: 0.0010\n",
            "Epoch 11/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.7982 - emotion_output_loss: 1.5678 - gender_output_loss: 0.2304 - emotion_output_accuracy: 0.3932 - gender_output_accuracy: 0.9056 - val_loss: 1.8353 - val_emotion_output_loss: 1.5491 - val_gender_output_loss: 0.2862 - val_emotion_output_accuracy: 0.4340 - val_gender_output_accuracy: 0.8819 - lr: 0.0010\n",
            "Epoch 12/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.7844 - emotion_output_loss: 1.5515 - gender_output_loss: 0.2329 - emotion_output_accuracy: 0.4001 - gender_output_accuracy: 0.9045 - val_loss: 1.7383 - val_emotion_output_loss: 1.5195 - val_gender_output_loss: 0.2187 - val_emotion_output_accuracy: 0.4531 - val_gender_output_accuracy: 0.9097 - lr: 0.0010\n",
            "Epoch 13/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.7416 - emotion_output_loss: 1.5351 - gender_output_loss: 0.2065 - emotion_output_accuracy: 0.4233 - gender_output_accuracy: 0.9201 - val_loss: 1.7461 - val_emotion_output_loss: 1.5124 - val_gender_output_loss: 0.2338 - val_emotion_output_accuracy: 0.4427 - val_gender_output_accuracy: 0.9045 - lr: 0.0010\n",
            "Epoch 14/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.7004 - emotion_output_loss: 1.4852 - gender_output_loss: 0.2152 - emotion_output_accuracy: 0.4325 - gender_output_accuracy: 0.9091 - val_loss: 1.7029 - val_emotion_output_loss: 1.4982 - val_gender_output_loss: 0.2047 - val_emotion_output_accuracy: 0.4444 - val_gender_output_accuracy: 0.9253 - lr: 0.0010\n",
            "Epoch 15/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.6361 - emotion_output_loss: 1.4315 - gender_output_loss: 0.2046 - emotion_output_accuracy: 0.4580 - gender_output_accuracy: 0.9149 - val_loss: 1.6496 - val_emotion_output_loss: 1.4449 - val_gender_output_loss: 0.2047 - val_emotion_output_accuracy: 0.4670 - val_gender_output_accuracy: 0.9306 - lr: 0.0010\n",
            "Epoch 16/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.6463 - emotion_output_loss: 1.4591 - gender_output_loss: 0.1872 - emotion_output_accuracy: 0.4435 - gender_output_accuracy: 0.9236 - val_loss: 1.7220 - val_emotion_output_loss: 1.5226 - val_gender_output_loss: 0.1994 - val_emotion_output_accuracy: 0.4323 - val_gender_output_accuracy: 0.9167 - lr: 0.0010\n",
            "Epoch 17/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.5561 - emotion_output_loss: 1.3774 - gender_output_loss: 0.1787 - emotion_output_accuracy: 0.4708 - gender_output_accuracy: 0.9270 - val_loss: 1.6037 - val_emotion_output_loss: 1.4135 - val_gender_output_loss: 0.1902 - val_emotion_output_accuracy: 0.4792 - val_gender_output_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 18/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.4928 - emotion_output_loss: 1.3348 - gender_output_loss: 0.1580 - emotion_output_accuracy: 0.4997 - gender_output_accuracy: 0.9415 - val_loss: 1.6470 - val_emotion_output_loss: 1.4529 - val_gender_output_loss: 0.1941 - val_emotion_output_accuracy: 0.4740 - val_gender_output_accuracy: 0.9392 - lr: 0.0010\n",
            "Epoch 19/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.5062 - emotion_output_loss: 1.3581 - gender_output_loss: 0.1481 - emotion_output_accuracy: 0.4887 - gender_output_accuracy: 0.9433 - val_loss: 1.5624 - val_emotion_output_loss: 1.3989 - val_gender_output_loss: 0.1635 - val_emotion_output_accuracy: 0.4878 - val_gender_output_accuracy: 0.9410 - lr: 0.0010\n",
            "Epoch 20/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.4626 - emotion_output_loss: 1.3165 - gender_output_loss: 0.1460 - emotion_output_accuracy: 0.5113 - gender_output_accuracy: 0.9386 - val_loss: 1.5161 - val_emotion_output_loss: 1.3422 - val_gender_output_loss: 0.1739 - val_emotion_output_accuracy: 0.5087 - val_gender_output_accuracy: 0.9410 - lr: 0.0010\n",
            "Epoch 21/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 1.4007 - emotion_output_loss: 1.2495 - gender_output_loss: 0.1513 - emotion_output_accuracy: 0.5345 - gender_output_accuracy: 0.9479 - val_loss: 1.5772 - val_emotion_output_loss: 1.3789 - val_gender_output_loss: 0.1983 - val_emotion_output_accuracy: 0.5052 - val_gender_output_accuracy: 0.9271 - lr: 0.0010\n",
            "Epoch 22/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 1.3444 - emotion_output_loss: 1.2063 - gender_output_loss: 0.1382 - emotion_output_accuracy: 0.5524 - gender_output_accuracy: 0.9467 - val_loss: 1.4566 - val_emotion_output_loss: 1.3058 - val_gender_output_loss: 0.1508 - val_emotion_output_accuracy: 0.5017 - val_gender_output_accuracy: 0.9462 - lr: 0.0010\n",
            "Epoch 23/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 1.3222 - emotion_output_loss: 1.1834 - gender_output_loss: 0.1387 - emotion_output_accuracy: 0.5518 - gender_output_accuracy: 0.9386 - val_loss: 1.4696 - val_emotion_output_loss: 1.2969 - val_gender_output_loss: 0.1727 - val_emotion_output_accuracy: 0.5365 - val_gender_output_accuracy: 0.9497 - lr: 0.0010\n",
            "Epoch 24/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.2265 - emotion_output_loss: 1.1215 - gender_output_loss: 0.1050 - emotion_output_accuracy: 0.5935 - gender_output_accuracy: 0.9629 - val_loss: 1.5215 - val_emotion_output_loss: 1.3439 - val_gender_output_loss: 0.1776 - val_emotion_output_accuracy: 0.5469 - val_gender_output_accuracy: 0.9392 - lr: 0.0010\n",
            "Epoch 25/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.2356 - emotion_output_loss: 1.1376 - gender_output_loss: 0.0980 - emotion_output_accuracy: 0.5715 - gender_output_accuracy: 0.9647 - val_loss: 1.4656 - val_emotion_output_loss: 1.3018 - val_gender_output_loss: 0.1638 - val_emotion_output_accuracy: 0.5278 - val_gender_output_accuracy: 0.9375 - lr: 0.0010\n",
            "Epoch 26/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 1.1788 - emotion_output_loss: 1.0731 - gender_output_loss: 0.1057 - emotion_output_accuracy: 0.6039 - gender_output_accuracy: 0.9612 - val_loss: 1.4490 - val_emotion_output_loss: 1.3044 - val_gender_output_loss: 0.1446 - val_emotion_output_accuracy: 0.5312 - val_gender_output_accuracy: 0.9531 - lr: 0.0010\n",
            "Epoch 27/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.1200 - emotion_output_loss: 1.0279 - gender_output_loss: 0.0921 - emotion_output_accuracy: 0.6340 - gender_output_accuracy: 0.9687 - val_loss: 1.4292 - val_emotion_output_loss: 1.2516 - val_gender_output_loss: 0.1776 - val_emotion_output_accuracy: 0.5642 - val_gender_output_accuracy: 0.9479 - lr: 0.0010\n",
            "Epoch 28/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.0992 - emotion_output_loss: 1.0040 - gender_output_loss: 0.0952 - emotion_output_accuracy: 0.6381 - gender_output_accuracy: 0.9658 - val_loss: 1.3669 - val_emotion_output_loss: 1.2270 - val_gender_output_loss: 0.1399 - val_emotion_output_accuracy: 0.5816 - val_gender_output_accuracy: 0.9462 - lr: 0.0010\n",
            "Epoch 29/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.9701 - emotion_output_loss: 0.8940 - gender_output_loss: 0.0761 - emotion_output_accuracy: 0.6740 - gender_output_accuracy: 0.9693 - val_loss: 1.3943 - val_emotion_output_loss: 1.2259 - val_gender_output_loss: 0.1684 - val_emotion_output_accuracy: 0.5851 - val_gender_output_accuracy: 0.9514 - lr: 0.0010\n",
            "Epoch 30/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 1.0085 - emotion_output_loss: 0.9196 - gender_output_loss: 0.0890 - emotion_output_accuracy: 0.6717 - gender_output_accuracy: 0.9641 - val_loss: 1.3823 - val_emotion_output_loss: 1.2070 - val_gender_output_loss: 0.1754 - val_emotion_output_accuracy: 0.6094 - val_gender_output_accuracy: 0.9497 - lr: 0.0010\n",
            "Epoch 31/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.9025 - emotion_output_loss: 0.8295 - gender_output_loss: 0.0730 - emotion_output_accuracy: 0.6891 - gender_output_accuracy: 0.9780 - val_loss: 1.3305 - val_emotion_output_loss: 1.1759 - val_gender_output_loss: 0.1546 - val_emotion_output_accuracy: 0.6163 - val_gender_output_accuracy: 0.9479 - lr: 0.0010\n",
            "Epoch 32/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.8667 - emotion_output_loss: 0.8106 - gender_output_loss: 0.0560 - emotion_output_accuracy: 0.7082 - gender_output_accuracy: 0.9809 - val_loss: 1.4651 - val_emotion_output_loss: 1.1994 - val_gender_output_loss: 0.2656 - val_emotion_output_accuracy: 0.6024 - val_gender_output_accuracy: 0.9253 - lr: 0.0010\n",
            "Epoch 33/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.8620 - emotion_output_loss: 0.7935 - gender_output_loss: 0.0684 - emotion_output_accuracy: 0.7087 - gender_output_accuracy: 0.9751 - val_loss: 1.3750 - val_emotion_output_loss: 1.2030 - val_gender_output_loss: 0.1720 - val_emotion_output_accuracy: 0.6111 - val_gender_output_accuracy: 0.9514 - lr: 0.0010\n",
            "Epoch 34/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.8471 - emotion_output_loss: 0.7785 - gender_output_loss: 0.0686 - emotion_output_accuracy: 0.7116 - gender_output_accuracy: 0.9751 - val_loss: 1.4876 - val_emotion_output_loss: 1.3171 - val_gender_output_loss: 0.1705 - val_emotion_output_accuracy: 0.6076 - val_gender_output_accuracy: 0.9462 - lr: 0.0010\n",
            "Epoch 35/250\n",
            "54/54 [==============================] - 1s 25ms/step - loss: 0.7770 - emotion_output_loss: 0.7201 - gender_output_loss: 0.0569 - emotion_output_accuracy: 0.7452 - gender_output_accuracy: 0.9803 - val_loss: 1.2856 - val_emotion_output_loss: 1.1372 - val_gender_output_loss: 0.1484 - val_emotion_output_accuracy: 0.6250 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 36/250\n",
            "54/54 [==============================] - 1s 25ms/step - loss: 0.7647 - emotion_output_loss: 0.7067 - gender_output_loss: 0.0580 - emotion_output_accuracy: 0.7412 - gender_output_accuracy: 0.9774 - val_loss: 1.3350 - val_emotion_output_loss: 1.1589 - val_gender_output_loss: 0.1761 - val_emotion_output_accuracy: 0.6458 - val_gender_output_accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 37/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.6941 - emotion_output_loss: 0.6426 - gender_output_loss: 0.0515 - emotion_output_accuracy: 0.7597 - gender_output_accuracy: 0.9838 - val_loss: 1.3154 - val_emotion_output_loss: 1.1540 - val_gender_output_loss: 0.1614 - val_emotion_output_accuracy: 0.6684 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 38/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.6712 - emotion_output_loss: 0.6290 - gender_output_loss: 0.0422 - emotion_output_accuracy: 0.7695 - gender_output_accuracy: 0.9849 - val_loss: 1.3287 - val_emotion_output_loss: 1.1538 - val_gender_output_loss: 0.1749 - val_emotion_output_accuracy: 0.6493 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 39/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.6212 - emotion_output_loss: 0.5613 - gender_output_loss: 0.0599 - emotion_output_accuracy: 0.7973 - gender_output_accuracy: 0.9792 - val_loss: 1.2750 - val_emotion_output_loss: 1.1119 - val_gender_output_loss: 0.1631 - val_emotion_output_accuracy: 0.6580 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 40/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.6352 - emotion_output_loss: 0.5877 - gender_output_loss: 0.0475 - emotion_output_accuracy: 0.7823 - gender_output_accuracy: 0.9832 - val_loss: 1.2337 - val_emotion_output_loss: 1.0735 - val_gender_output_loss: 0.1602 - val_emotion_output_accuracy: 0.6545 - val_gender_output_accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 41/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.6349 - emotion_output_loss: 0.5799 - gender_output_loss: 0.0550 - emotion_output_accuracy: 0.7910 - gender_output_accuracy: 0.9809 - val_loss: 1.2063 - val_emotion_output_loss: 1.0381 - val_gender_output_loss: 0.1682 - val_emotion_output_accuracy: 0.6927 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 42/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.6120 - emotion_output_loss: 0.5704 - gender_output_loss: 0.0416 - emotion_output_accuracy: 0.7950 - gender_output_accuracy: 0.9849 - val_loss: 1.3549 - val_emotion_output_loss: 1.1653 - val_gender_output_loss: 0.1897 - val_emotion_output_accuracy: 0.6510 - val_gender_output_accuracy: 0.9479 - lr: 0.0010\n",
            "Epoch 43/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.5717 - emotion_output_loss: 0.5213 - gender_output_loss: 0.0504 - emotion_output_accuracy: 0.8112 - gender_output_accuracy: 0.9820 - val_loss: 1.2390 - val_emotion_output_loss: 1.0589 - val_gender_output_loss: 0.1801 - val_emotion_output_accuracy: 0.6962 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 44/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.5298 - emotion_output_loss: 0.4805 - gender_output_loss: 0.0493 - emotion_output_accuracy: 0.8228 - gender_output_accuracy: 0.9815 - val_loss: 1.2960 - val_emotion_output_loss: 1.0755 - val_gender_output_loss: 0.2205 - val_emotion_output_accuracy: 0.6823 - val_gender_output_accuracy: 0.9479 - lr: 0.0010\n",
            "Epoch 45/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.4644 - emotion_output_loss: 0.4355 - gender_output_loss: 0.0288 - emotion_output_accuracy: 0.8402 - gender_output_accuracy: 0.9907 - val_loss: 1.2664 - val_emotion_output_loss: 1.1099 - val_gender_output_loss: 0.1565 - val_emotion_output_accuracy: 0.7118 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 46/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.4266 - emotion_output_loss: 0.4008 - gender_output_loss: 0.0258 - emotion_output_accuracy: 0.8570 - gender_output_accuracy: 0.9907 - val_loss: 1.3149 - val_emotion_output_loss: 1.1235 - val_gender_output_loss: 0.1914 - val_emotion_output_accuracy: 0.7049 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 47/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.4328 - emotion_output_loss: 0.3945 - gender_output_loss: 0.0383 - emotion_output_accuracy: 0.8610 - gender_output_accuracy: 0.9867 - val_loss: 1.2576 - val_emotion_output_loss: 1.1125 - val_gender_output_loss: 0.1451 - val_emotion_output_accuracy: 0.6979 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 48/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.4506 - emotion_output_loss: 0.4014 - gender_output_loss: 0.0491 - emotion_output_accuracy: 0.8610 - gender_output_accuracy: 0.9815 - val_loss: 1.2201 - val_emotion_output_loss: 1.0608 - val_gender_output_loss: 0.1593 - val_emotion_output_accuracy: 0.7170 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 49/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.3840 - emotion_output_loss: 0.3555 - gender_output_loss: 0.0286 - emotion_output_accuracy: 0.8691 - gender_output_accuracy: 0.9873 - val_loss: 1.2279 - val_emotion_output_loss: 1.0489 - val_gender_output_loss: 0.1791 - val_emotion_output_accuracy: 0.7344 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 50/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.3624 - emotion_output_loss: 0.3367 - gender_output_loss: 0.0257 - emotion_output_accuracy: 0.8796 - gender_output_accuracy: 0.9907 - val_loss: 1.3177 - val_emotion_output_loss: 1.1316 - val_gender_output_loss: 0.1861 - val_emotion_output_accuracy: 0.7292 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 51/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 0.3587 - emotion_output_loss: 0.3187 - gender_output_loss: 0.0400 - emotion_output_accuracy: 0.8917 - gender_output_accuracy: 0.9884 - val_loss: 1.4097 - val_emotion_output_loss: 1.2202 - val_gender_output_loss: 0.1895 - val_emotion_output_accuracy: 0.7309 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 52/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.3681 - emotion_output_loss: 0.3399 - gender_output_loss: 0.0282 - emotion_output_accuracy: 0.8761 - gender_output_accuracy: 0.9884 - val_loss: 1.2865 - val_emotion_output_loss: 1.1386 - val_gender_output_loss: 0.1478 - val_emotion_output_accuracy: 0.7031 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 53/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.3265 - emotion_output_loss: 0.3055 - gender_output_loss: 0.0210 - emotion_output_accuracy: 0.8946 - gender_output_accuracy: 0.9925 - val_loss: 1.4906 - val_emotion_output_loss: 1.2593 - val_gender_output_loss: 0.2313 - val_emotion_output_accuracy: 0.7083 - val_gender_output_accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 54/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.3182 - emotion_output_loss: 0.2904 - gender_output_loss: 0.0278 - emotion_output_accuracy: 0.9027 - gender_output_accuracy: 0.9890 - val_loss: 1.3568 - val_emotion_output_loss: 1.1666 - val_gender_output_loss: 0.1902 - val_emotion_output_accuracy: 0.7274 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 55/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.4017 - emotion_output_loss: 0.3752 - gender_output_loss: 0.0265 - emotion_output_accuracy: 0.8738 - gender_output_accuracy: 0.9913 - val_loss: 1.4100 - val_emotion_output_loss: 1.1848 - val_gender_output_loss: 0.2253 - val_emotion_output_accuracy: 0.7118 - val_gender_output_accuracy: 0.9479 - lr: 0.0010\n",
            "Epoch 56/250\n",
            "54/54 [==============================] - 1s 26ms/step - loss: 0.3345 - emotion_output_loss: 0.3107 - gender_output_loss: 0.0238 - emotion_output_accuracy: 0.8911 - gender_output_accuracy: 0.9907 - val_loss: 1.3655 - val_emotion_output_loss: 1.1621 - val_gender_output_loss: 0.2034 - val_emotion_output_accuracy: 0.7378 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 57/250\n",
            "54/54 [==============================] - 1s 20ms/step - loss: 0.2765 - emotion_output_loss: 0.2612 - gender_output_loss: 0.0154 - emotion_output_accuracy: 0.9056 - gender_output_accuracy: 0.9954 - val_loss: 1.4284 - val_emotion_output_loss: 1.2113 - val_gender_output_loss: 0.2171 - val_emotion_output_accuracy: 0.7326 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 58/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.2681 - emotion_output_loss: 0.2488 - gender_output_loss: 0.0193 - emotion_output_accuracy: 0.9126 - gender_output_accuracy: 0.9936 - val_loss: 1.6242 - val_emotion_output_loss: 1.4274 - val_gender_output_loss: 0.1968 - val_emotion_output_accuracy: 0.7066 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 59/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.3034 - emotion_output_loss: 0.2816 - gender_output_loss: 0.0218 - emotion_output_accuracy: 0.9074 - gender_output_accuracy: 0.9931 - val_loss: 1.5568 - val_emotion_output_loss: 1.3277 - val_gender_output_loss: 0.2291 - val_emotion_output_accuracy: 0.7292 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 60/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.2745 - emotion_output_loss: 0.2447 - gender_output_loss: 0.0298 - emotion_output_accuracy: 0.9201 - gender_output_accuracy: 0.9913 - val_loss: 1.6166 - val_emotion_output_loss: 1.4058 - val_gender_output_loss: 0.2108 - val_emotion_output_accuracy: 0.7361 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 61/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 0.3259 - emotion_output_loss: 0.3044 - gender_output_loss: 0.0215 - emotion_output_accuracy: 0.9050 - gender_output_accuracy: 0.9942 - val_loss: 1.3996 - val_emotion_output_loss: 1.1955 - val_gender_output_loss: 0.2041 - val_emotion_output_accuracy: 0.7413 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 62/250\n",
            "54/54 [==============================] - 1s 27ms/step - loss: 0.2838 - emotion_output_loss: 0.2624 - gender_output_loss: 0.0214 - emotion_output_accuracy: 0.9079 - gender_output_accuracy: 0.9931 - val_loss: 1.4914 - val_emotion_output_loss: 1.2701 - val_gender_output_loss: 0.2213 - val_emotion_output_accuracy: 0.7517 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 63/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.2625 - emotion_output_loss: 0.2297 - gender_output_loss: 0.0329 - emotion_output_accuracy: 0.9236 - gender_output_accuracy: 0.9884 - val_loss: 1.5707 - val_emotion_output_loss: 1.2903 - val_gender_output_loss: 0.2804 - val_emotion_output_accuracy: 0.7517 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 64/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2113 - emotion_output_loss: 0.1935 - gender_output_loss: 0.0179 - emotion_output_accuracy: 0.9369 - gender_output_accuracy: 0.9907 - val_loss: 1.5255 - val_emotion_output_loss: 1.3032 - val_gender_output_loss: 0.2223 - val_emotion_output_accuracy: 0.7465 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 65/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2295 - emotion_output_loss: 0.2113 - gender_output_loss: 0.0182 - emotion_output_accuracy: 0.9323 - gender_output_accuracy: 0.9936 - val_loss: 1.5337 - val_emotion_output_loss: 1.2872 - val_gender_output_loss: 0.2465 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9497 - lr: 0.0010\n",
            "Epoch 66/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1886 - emotion_output_loss: 0.1689 - gender_output_loss: 0.0197 - emotion_output_accuracy: 0.9444 - gender_output_accuracy: 0.9919 - val_loss: 1.5380 - val_emotion_output_loss: 1.2961 - val_gender_output_loss: 0.2419 - val_emotion_output_accuracy: 0.7760 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 67/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.1423 - emotion_output_loss: 0.1188 - gender_output_loss: 0.0234 - emotion_output_accuracy: 0.9577 - gender_output_accuracy: 0.9907 - val_loss: 1.5580 - val_emotion_output_loss: 1.2777 - val_gender_output_loss: 0.2803 - val_emotion_output_accuracy: 0.7431 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 68/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1319 - emotion_output_loss: 0.1209 - gender_output_loss: 0.0110 - emotion_output_accuracy: 0.9612 - gender_output_accuracy: 0.9954 - val_loss: 1.6391 - val_emotion_output_loss: 1.3595 - val_gender_output_loss: 0.2796 - val_emotion_output_accuracy: 0.7622 - val_gender_output_accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 69/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.2598 - emotion_output_loss: 0.2044 - gender_output_loss: 0.0555 - emotion_output_accuracy: 0.9369 - gender_output_accuracy: 0.9867 - val_loss: 1.5212 - val_emotion_output_loss: 1.2972 - val_gender_output_loss: 0.2240 - val_emotion_output_accuracy: 0.7431 - val_gender_output_accuracy: 0.9531 - lr: 0.0010\n",
            "Epoch 70/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.3381 - emotion_output_loss: 0.3023 - gender_output_loss: 0.0359 - emotion_output_accuracy: 0.9027 - gender_output_accuracy: 0.9861 - val_loss: 1.4434 - val_emotion_output_loss: 1.2865 - val_gender_output_loss: 0.1569 - val_emotion_output_accuracy: 0.7309 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 71/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.2573 - emotion_output_loss: 0.2223 - gender_output_loss: 0.0350 - emotion_output_accuracy: 0.9270 - gender_output_accuracy: 0.9919 - val_loss: 1.4788 - val_emotion_output_loss: 1.2733 - val_gender_output_loss: 0.2055 - val_emotion_output_accuracy: 0.7517 - val_gender_output_accuracy: 0.9531 - lr: 0.0010\n",
            "Epoch 72/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1619 - emotion_output_loss: 0.1459 - gender_output_loss: 0.0160 - emotion_output_accuracy: 0.9519 - gender_output_accuracy: 0.9954 - val_loss: 1.4459 - val_emotion_output_loss: 1.2656 - val_gender_output_loss: 0.1802 - val_emotion_output_accuracy: 0.7760 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 73/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1474 - emotion_output_loss: 0.1381 - gender_output_loss: 0.0093 - emotion_output_accuracy: 0.9525 - gender_output_accuracy: 0.9965 - val_loss: 1.5495 - val_emotion_output_loss: 1.3423 - val_gender_output_loss: 0.2072 - val_emotion_output_accuracy: 0.7622 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 74/250\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.1433 - emotion_output_loss: 0.1283 - gender_output_loss: 0.0150 - emotion_output_accuracy: 0.9554 - gender_output_accuracy: 0.9948 - val_loss: 1.7785 - val_emotion_output_loss: 1.5201 - val_gender_output_loss: 0.2584 - val_emotion_output_accuracy: 0.7500 - val_gender_output_accuracy: 0.9531 - lr: 0.0010\n",
            "Epoch 75/250\n",
            "54/54 [==============================] - 1s 26ms/step - loss: 0.1134 - emotion_output_loss: 0.0966 - gender_output_loss: 0.0168 - emotion_output_accuracy: 0.9699 - gender_output_accuracy: 0.9942 - val_loss: 1.6768 - val_emotion_output_loss: 1.4744 - val_gender_output_loss: 0.2024 - val_emotion_output_accuracy: 0.7743 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 76/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 0.1155 - emotion_output_loss: 0.1055 - gender_output_loss: 0.0100 - emotion_output_accuracy: 0.9693 - gender_output_accuracy: 0.9948 - val_loss: 1.5632 - val_emotion_output_loss: 1.3803 - val_gender_output_loss: 0.1829 - val_emotion_output_accuracy: 0.7743 - val_gender_output_accuracy: 0.9740 - lr: 0.0010\n",
            "Epoch 77/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.2229 - emotion_output_loss: 0.2011 - gender_output_loss: 0.0218 - emotion_output_accuracy: 0.9357 - gender_output_accuracy: 0.9942 - val_loss: 1.7102 - val_emotion_output_loss: 1.5161 - val_gender_output_loss: 0.1941 - val_emotion_output_accuracy: 0.7535 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 78/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.2513 - emotion_output_loss: 0.2223 - gender_output_loss: 0.0289 - emotion_output_accuracy: 0.9259 - gender_output_accuracy: 0.9878 - val_loss: 1.4132 - val_emotion_output_loss: 1.2295 - val_gender_output_loss: 0.1838 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 79/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.2529 - emotion_output_loss: 0.2240 - gender_output_loss: 0.0289 - emotion_output_accuracy: 0.9305 - gender_output_accuracy: 0.9907 - val_loss: 1.4283 - val_emotion_output_loss: 1.2624 - val_gender_output_loss: 0.1659 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9722 - lr: 0.0010\n",
            "Epoch 80/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1426 - emotion_output_loss: 0.1281 - gender_output_loss: 0.0144 - emotion_output_accuracy: 0.9606 - gender_output_accuracy: 0.9948 - val_loss: 1.5344 - val_emotion_output_loss: 1.3596 - val_gender_output_loss: 0.1749 - val_emotion_output_accuracy: 0.7622 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 81/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1975 - emotion_output_loss: 0.1806 - gender_output_loss: 0.0170 - emotion_output_accuracy: 0.9433 - gender_output_accuracy: 0.9936 - val_loss: 1.5457 - val_emotion_output_loss: 1.3859 - val_gender_output_loss: 0.1598 - val_emotion_output_accuracy: 0.7326 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 82/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1640 - emotion_output_loss: 0.1490 - gender_output_loss: 0.0150 - emotion_output_accuracy: 0.9519 - gender_output_accuracy: 0.9931 - val_loss: 1.5726 - val_emotion_output_loss: 1.3935 - val_gender_output_loss: 0.1791 - val_emotion_output_accuracy: 0.7656 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 83/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1617 - emotion_output_loss: 0.1439 - gender_output_loss: 0.0178 - emotion_output_accuracy: 0.9548 - gender_output_accuracy: 0.9948 - val_loss: 1.6383 - val_emotion_output_loss: 1.4190 - val_gender_output_loss: 0.2193 - val_emotion_output_accuracy: 0.7500 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 84/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1916 - emotion_output_loss: 0.1685 - gender_output_loss: 0.0231 - emotion_output_accuracy: 0.9577 - gender_output_accuracy: 0.9925 - val_loss: 1.4227 - val_emotion_output_loss: 1.2579 - val_gender_output_loss: 0.1648 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 85/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1119 - emotion_output_loss: 0.0999 - gender_output_loss: 0.0120 - emotion_output_accuracy: 0.9728 - gender_output_accuracy: 0.9959 - val_loss: 1.7604 - val_emotion_output_loss: 1.5919 - val_gender_output_loss: 0.1685 - val_emotion_output_accuracy: 0.7344 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 86/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1387 - emotion_output_loss: 0.1325 - gender_output_loss: 0.0062 - emotion_output_accuracy: 0.9514 - gender_output_accuracy: 0.9977 - val_loss: 1.6681 - val_emotion_output_loss: 1.4310 - val_gender_output_loss: 0.2371 - val_emotion_output_accuracy: 0.7674 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 87/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1205 - emotion_output_loss: 0.1035 - gender_output_loss: 0.0171 - emotion_output_accuracy: 0.9710 - gender_output_accuracy: 0.9954 - val_loss: 1.6110 - val_emotion_output_loss: 1.4111 - val_gender_output_loss: 0.1999 - val_emotion_output_accuracy: 0.7622 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 88/250\n",
            "54/54 [==============================] - 1s 25ms/step - loss: 0.0946 - emotion_output_loss: 0.0845 - gender_output_loss: 0.0102 - emotion_output_accuracy: 0.9751 - gender_output_accuracy: 0.9965 - val_loss: 1.6439 - val_emotion_output_loss: 1.4202 - val_gender_output_loss: 0.2237 - val_emotion_output_accuracy: 0.7726 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 89/250\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.1196 - emotion_output_loss: 0.1111 - gender_output_loss: 0.0085 - emotion_output_accuracy: 0.9676 - gender_output_accuracy: 0.9965 - val_loss: 1.6225 - val_emotion_output_loss: 1.4103 - val_gender_output_loss: 0.2122 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 90/250\n",
            "54/54 [==============================] - 1s 25ms/step - loss: 0.0982 - emotion_output_loss: 0.0890 - gender_output_loss: 0.0092 - emotion_output_accuracy: 0.9699 - gender_output_accuracy: 0.9965 - val_loss: 1.6563 - val_emotion_output_loss: 1.4440 - val_gender_output_loss: 0.2123 - val_emotion_output_accuracy: 0.7847 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 91/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.0643 - emotion_output_loss: 0.0614 - gender_output_loss: 0.0028 - emotion_output_accuracy: 0.9844 - gender_output_accuracy: 1.0000 - val_loss: 1.7302 - val_emotion_output_loss: 1.5222 - val_gender_output_loss: 0.2080 - val_emotion_output_accuracy: 0.7674 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 92/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0679 - emotion_output_loss: 0.0630 - gender_output_loss: 0.0049 - emotion_output_accuracy: 0.9832 - gender_output_accuracy: 0.9977 - val_loss: 1.7723 - val_emotion_output_loss: 1.4717 - val_gender_output_loss: 0.3005 - val_emotion_output_accuracy: 0.7674 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 93/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.0560 - emotion_output_loss: 0.0503 - gender_output_loss: 0.0057 - emotion_output_accuracy: 0.9832 - gender_output_accuracy: 0.9971 - val_loss: 1.6921 - val_emotion_output_loss: 1.4576 - val_gender_output_loss: 0.2346 - val_emotion_output_accuracy: 0.7622 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 94/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1071 - emotion_output_loss: 0.0968 - gender_output_loss: 0.0102 - emotion_output_accuracy: 0.9786 - gender_output_accuracy: 0.9977 - val_loss: 1.6859 - val_emotion_output_loss: 1.4136 - val_gender_output_loss: 0.2723 - val_emotion_output_accuracy: 0.7882 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 95/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1526 - emotion_output_loss: 0.1366 - gender_output_loss: 0.0160 - emotion_output_accuracy: 0.9618 - gender_output_accuracy: 0.9948 - val_loss: 1.6284 - val_emotion_output_loss: 1.4066 - val_gender_output_loss: 0.2218 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 96/250\n",
            "54/54 [==============================] - 1s 20ms/step - loss: 0.0877 - emotion_output_loss: 0.0766 - gender_output_loss: 0.0112 - emotion_output_accuracy: 0.9751 - gender_output_accuracy: 0.9948 - val_loss: 1.6751 - val_emotion_output_loss: 1.4501 - val_gender_output_loss: 0.2250 - val_emotion_output_accuracy: 0.7674 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 97/250\n",
            "54/54 [==============================] - 1s 20ms/step - loss: 0.1016 - emotion_output_loss: 0.0898 - gender_output_loss: 0.0118 - emotion_output_accuracy: 0.9728 - gender_output_accuracy: 0.9965 - val_loss: 1.8992 - val_emotion_output_loss: 1.6382 - val_gender_output_loss: 0.2611 - val_emotion_output_accuracy: 0.7674 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 98/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.1257 - emotion_output_loss: 0.0998 - gender_output_loss: 0.0259 - emotion_output_accuracy: 0.9699 - gender_output_accuracy: 0.9931 - val_loss: 1.7710 - val_emotion_output_loss: 1.4894 - val_gender_output_loss: 0.2816 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9549 - lr: 0.0010\n",
            "Epoch 99/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.1259 - emotion_output_loss: 0.1004 - gender_output_loss: 0.0254 - emotion_output_accuracy: 0.9687 - gender_output_accuracy: 0.9907 - val_loss: 1.7908 - val_emotion_output_loss: 1.5496 - val_gender_output_loss: 0.2413 - val_emotion_output_accuracy: 0.7587 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 100/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.1013 - emotion_output_loss: 0.0919 - gender_output_loss: 0.0094 - emotion_output_accuracy: 0.9751 - gender_output_accuracy: 0.9971 - val_loss: 1.7184 - val_emotion_output_loss: 1.4747 - val_gender_output_loss: 0.2437 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9618 - lr: 0.0010\n",
            "Epoch 101/250\n",
            "54/54 [==============================] - 1s 26ms/step - loss: 0.0852 - emotion_output_loss: 0.0772 - gender_output_loss: 0.0080 - emotion_output_accuracy: 0.9780 - gender_output_accuracy: 0.9977 - val_loss: 1.6068 - val_emotion_output_loss: 1.4054 - val_gender_output_loss: 0.2013 - val_emotion_output_accuracy: 0.7847 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 102/250\n",
            "54/54 [==============================] - 1s 25ms/step - loss: 0.1054 - emotion_output_loss: 0.0814 - gender_output_loss: 0.0240 - emotion_output_accuracy: 0.9722 - gender_output_accuracy: 0.9931 - val_loss: 1.7261 - val_emotion_output_loss: 1.5342 - val_gender_output_loss: 0.1919 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 103/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.1631 - emotion_output_loss: 0.1404 - gender_output_loss: 0.0228 - emotion_output_accuracy: 0.9577 - gender_output_accuracy: 0.9907 - val_loss: 1.7459 - val_emotion_output_loss: 1.4872 - val_gender_output_loss: 0.2587 - val_emotion_output_accuracy: 0.7604 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 104/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1968 - emotion_output_loss: 0.1815 - gender_output_loss: 0.0153 - emotion_output_accuracy: 0.9444 - gender_output_accuracy: 0.9959 - val_loss: 1.6886 - val_emotion_output_loss: 1.5016 - val_gender_output_loss: 0.1870 - val_emotion_output_accuracy: 0.7431 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 105/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1103 - emotion_output_loss: 0.1047 - gender_output_loss: 0.0056 - emotion_output_accuracy: 0.9734 - gender_output_accuracy: 0.9988 - val_loss: 1.7734 - val_emotion_output_loss: 1.5489 - val_gender_output_loss: 0.2245 - val_emotion_output_accuracy: 0.7917 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 106/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0758 - emotion_output_loss: 0.0670 - gender_output_loss: 0.0087 - emotion_output_accuracy: 0.9820 - gender_output_accuracy: 0.9983 - val_loss: 1.8420 - val_emotion_output_loss: 1.5867 - val_gender_output_loss: 0.2552 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 107/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0554 - emotion_output_loss: 0.0480 - gender_output_loss: 0.0074 - emotion_output_accuracy: 0.9844 - gender_output_accuracy: 0.9977 - val_loss: 1.9790 - val_emotion_output_loss: 1.6997 - val_gender_output_loss: 0.2793 - val_emotion_output_accuracy: 0.7552 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 108/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0616 - emotion_output_loss: 0.0480 - gender_output_loss: 0.0136 - emotion_output_accuracy: 0.9861 - gender_output_accuracy: 0.9948 - val_loss: 1.7952 - val_emotion_output_loss: 1.5520 - val_gender_output_loss: 0.2432 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9705 - lr: 0.0010\n",
            "Epoch 109/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0792 - emotion_output_loss: 0.0630 - gender_output_loss: 0.0162 - emotion_output_accuracy: 0.9809 - gender_output_accuracy: 0.9954 - val_loss: 1.6888 - val_emotion_output_loss: 1.4917 - val_gender_output_loss: 0.1971 - val_emotion_output_accuracy: 0.7726 - val_gender_output_accuracy: 0.9774 - lr: 0.0010\n",
            "Epoch 110/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0899 - emotion_output_loss: 0.0832 - gender_output_loss: 0.0067 - emotion_output_accuracy: 0.9734 - gender_output_accuracy: 0.9988 - val_loss: 1.8238 - val_emotion_output_loss: 1.6074 - val_gender_output_loss: 0.2163 - val_emotion_output_accuracy: 0.7535 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n",
            "Epoch 111/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.1481 - emotion_output_loss: 0.1292 - gender_output_loss: 0.0188 - emotion_output_accuracy: 0.9664 - gender_output_accuracy: 0.9948 - val_loss: 1.7754 - val_emotion_output_loss: 1.5430 - val_gender_output_loss: 0.2324 - val_emotion_output_accuracy: 0.7535 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 112/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.1202 - emotion_output_loss: 0.0984 - gender_output_loss: 0.0218 - emotion_output_accuracy: 0.9693 - gender_output_accuracy: 0.9942 - val_loss: 1.5252 - val_emotion_output_loss: 1.3747 - val_gender_output_loss: 0.1505 - val_emotion_output_accuracy: 0.7760 - val_gender_output_accuracy: 0.9740 - lr: 0.0010\n",
            "Epoch 113/250\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.1041 - emotion_output_loss: 0.0917 - gender_output_loss: 0.0123 - emotion_output_accuracy: 0.9705 - gender_output_accuracy: 0.9954 - val_loss: 1.7619 - val_emotion_output_loss: 1.4955 - val_gender_output_loss: 0.2664 - val_emotion_output_accuracy: 0.7778 - val_gender_output_accuracy: 0.9497 - lr: 0.0010\n",
            "Epoch 114/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.1229 - emotion_output_loss: 0.1111 - gender_output_loss: 0.0118 - emotion_output_accuracy: 0.9653 - gender_output_accuracy: 0.9959 - val_loss: 1.7761 - val_emotion_output_loss: 1.5290 - val_gender_output_loss: 0.2471 - val_emotion_output_accuracy: 0.7795 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 115/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.2152 - emotion_output_loss: 0.2044 - gender_output_loss: 0.0108 - emotion_output_accuracy: 0.9433 - gender_output_accuracy: 0.9954 - val_loss: 1.6821 - val_emotion_output_loss: 1.4412 - val_gender_output_loss: 0.2410 - val_emotion_output_accuracy: 0.7431 - val_gender_output_accuracy: 0.9566 - lr: 0.0010\n",
            "Epoch 116/250\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.1789 - emotion_output_loss: 0.1725 - gender_output_loss: 0.0064 - emotion_output_accuracy: 0.9427 - gender_output_accuracy: 0.9988 - val_loss: 1.8092 - val_emotion_output_loss: 1.5736 - val_gender_output_loss: 0.2355 - val_emotion_output_accuracy: 0.7604 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 117/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0968 - emotion_output_loss: 0.0863 - gender_output_loss: 0.0105 - emotion_output_accuracy: 0.9716 - gender_output_accuracy: 0.9965 - val_loss: 1.6661 - val_emotion_output_loss: 1.4734 - val_gender_output_loss: 0.1927 - val_emotion_output_accuracy: 0.7778 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 118/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0814 - emotion_output_loss: 0.0749 - gender_output_loss: 0.0065 - emotion_output_accuracy: 0.9820 - gender_output_accuracy: 0.9977 - val_loss: 1.6986 - val_emotion_output_loss: 1.4938 - val_gender_output_loss: 0.2048 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9740 - lr: 0.0010\n",
            "Epoch 119/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0899 - emotion_output_loss: 0.0855 - gender_output_loss: 0.0044 - emotion_output_accuracy: 0.9745 - gender_output_accuracy: 0.9988 - val_loss: 1.7977 - val_emotion_output_loss: 1.6238 - val_gender_output_loss: 0.1739 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9774 - lr: 0.0010\n",
            "Epoch 120/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.1378 - emotion_output_loss: 0.1253 - gender_output_loss: 0.0125 - emotion_output_accuracy: 0.9670 - gender_output_accuracy: 0.9959 - val_loss: 1.6788 - val_emotion_output_loss: 1.4771 - val_gender_output_loss: 0.2017 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 121/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0751 - emotion_output_loss: 0.0650 - gender_output_loss: 0.0101 - emotion_output_accuracy: 0.9820 - gender_output_accuracy: 0.9965 - val_loss: 1.8044 - val_emotion_output_loss: 1.5779 - val_gender_output_loss: 0.2264 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9722 - lr: 0.0010\n",
            "Epoch 122/250\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.0959 - emotion_output_loss: 0.0835 - gender_output_loss: 0.0124 - emotion_output_accuracy: 0.9792 - gender_output_accuracy: 0.9971 - val_loss: 1.6953 - val_emotion_output_loss: 1.4712 - val_gender_output_loss: 0.2241 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 123/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0703 - emotion_output_loss: 0.0661 - gender_output_loss: 0.0042 - emotion_output_accuracy: 0.9832 - gender_output_accuracy: 0.9983 - val_loss: 1.7300 - val_emotion_output_loss: 1.4910 - val_gender_output_loss: 0.2390 - val_emotion_output_accuracy: 0.7743 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 124/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0458 - emotion_output_loss: 0.0406 - gender_output_loss: 0.0052 - emotion_output_accuracy: 0.9878 - gender_output_accuracy: 0.9977 - val_loss: 1.7309 - val_emotion_output_loss: 1.5041 - val_gender_output_loss: 0.2268 - val_emotion_output_accuracy: 0.7743 - val_gender_output_accuracy: 0.9688 - lr: 0.0010\n",
            "Epoch 125/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0315 - emotion_output_loss: 0.0268 - gender_output_loss: 0.0047 - emotion_output_accuracy: 0.9931 - gender_output_accuracy: 0.9977 - val_loss: 1.8597 - val_emotion_output_loss: 1.6184 - val_gender_output_loss: 0.2412 - val_emotion_output_accuracy: 0.7708 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 126/250\n",
            "54/54 [==============================] - 1s 18ms/step - loss: 0.0718 - emotion_output_loss: 0.0556 - gender_output_loss: 0.0161 - emotion_output_accuracy: 0.9809 - gender_output_accuracy: 0.9965 - val_loss: 1.9422 - val_emotion_output_loss: 1.6768 - val_gender_output_loss: 0.2654 - val_emotion_output_accuracy: 0.7812 - val_gender_output_accuracy: 0.9583 - lr: 0.0010\n",
            "Epoch 127/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.0535 - emotion_output_loss: 0.0484 - gender_output_loss: 0.0051 - emotion_output_accuracy: 0.9861 - gender_output_accuracy: 0.9983 - val_loss: 1.8183 - val_emotion_output_loss: 1.5940 - val_gender_output_loss: 0.2243 - val_emotion_output_accuracy: 0.7691 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 128/250\n",
            "54/54 [==============================] - 1s 27ms/step - loss: 0.0756 - emotion_output_loss: 0.0593 - gender_output_loss: 0.0162 - emotion_output_accuracy: 0.9832 - gender_output_accuracy: 0.9959 - val_loss: 1.8484 - val_emotion_output_loss: 1.6624 - val_gender_output_loss: 0.1860 - val_emotion_output_accuracy: 0.7760 - val_gender_output_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 129/250\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 0.0654 - emotion_output_loss: 0.0546 - gender_output_loss: 0.0108 - emotion_output_accuracy: 0.9815 - gender_output_accuracy: 0.9954 - val_loss: 1.7588 - val_emotion_output_loss: 1.5640 - val_gender_output_loss: 0.1948 - val_emotion_output_accuracy: 0.7778 - val_gender_output_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 130/250\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.1404 - emotion_output_loss: 0.1157 - gender_output_loss: 0.0247 - emotion_output_accuracy: 0.9664 - gender_output_accuracy: 0.9931 - val_loss: 1.7419 - val_emotion_output_loss: 1.5739 - val_gender_output_loss: 0.1679 - val_emotion_output_accuracy: 0.7639 - val_gender_output_accuracy: 0.9635 - lr: 0.0010\n",
            "Epoch 131/250\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.0927 - emotion_output_loss: 0.0839 - gender_output_loss: 0.0088 - emotion_output_accuracy: 0.9722 - gender_output_accuracy: 0.9965 - val_loss: 1.6714 - val_emotion_output_loss: 1.4955 - val_gender_output_loss: 0.1759 - val_emotion_output_accuracy: 0.7934 - val_gender_output_accuracy: 0.9670 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "# Create the model\n",
        "model_t = lstm_model()\n",
        "\n",
        "# Train the model\n",
        "history = model_t.fit(\n",
        "    np.expand_dims(x_train, -1),\n",
        "    {\"emotion_output\": y_train_emotion, \"gender_output\": y_train_gender},\n",
        "    validation_data=(np.expand_dims(x_val, -1), {\"emotion_output\": y_val_emotion, \"gender_output\": y_val_gender}),\n",
        "    epochs=250,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1toeYBYIjbS"
      },
      "outputs": [],
      "source": [
        "model_t.save(\"emotion_acc-82.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPqnfwg2-5lb",
        "outputId": "a1a662e2-22e5-4a42-c27d-dd88e67979c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 9ms/step - loss: 1.7904 - emotion_output_loss: 1.6144 - gender_output_loss: 0.1760 - emotion_output_accuracy: 0.8125 - gender_output_accuracy: 0.9722\n",
            "Test metrics: [1.7904330492019653, 1.6144070625305176, 0.1760261207818985, 0.8125, 0.9722222089767456]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate on test set\n",
        "test_metrics = model_t.evaluate(\n",
        "    np.expand_dims(x_test, -1),\n",
        "    {\"emotion_output\": y_test_emotion, \"gender_output\": y_test_gender}\n",
        ")\n",
        "print(f\"Test metrics: {test_metrics}\")\n",
        "\n",
        "# Emotion and Gender labels\n",
        "emotions = {1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad', 5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'}\n",
        "genders = {0: 'female', 1: 'male'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "JoxSh_QQ6mgd",
        "outputId": "daa23e03-e823-4ad9-bd70-19a2a742e5d4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4485256e8c10>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/sih_accuracy/emotion_acc-82.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'load_model'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "load_model = tf.keras.load_model(\"/content/drive/MyDrive/sih_accuracy/emotion_acc-82.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuILrQwF_Lni"
      },
      "outputs": [],
      "source": [
        "def predict(wavfile_testname):\n",
        "    test_point = mfcc_extract(wavfile_testname)\n",
        "    test_point = np.reshape(test_point, newshape=(1, 40, 1))\n",
        "    predictions = model_t.predict(test_point)\n",
        "\n",
        "    print(\"Emotion:\", emotions[np.argmax(predictions[0]) + 1])\n",
        "    print(\"Gender:\", genders[np.argmax(predictions[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sE1Km2mjX9z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e976EUn96OIA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owp13LgXjYco"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itbnXrF3yGG_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8i_DRTSjY9n"
      },
      "outputs": [],
      "source": [
        "# Emotion labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0MMBI4-jZRq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHQxKVBqjZme"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cAZOiSFjZ44"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z84b5bdpjaVV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi_NdbY8jbFi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GUaHEMRjbaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZwWTW1fjbsg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvNQZPzSjb-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo8Qjj5Fjcay"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psGfBaxCjAW1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1K-DGfRjKrH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
